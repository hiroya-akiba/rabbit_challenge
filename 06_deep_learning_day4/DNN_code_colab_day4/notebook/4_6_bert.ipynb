{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"4_9_bert.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"id":"OnyQzhRRLCZK"},"source":["!pip install mecab-python3\n","!pip install unidic\n","!python -m unidic download\n","!pip install fugashi\n","!pip install ipadic"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"24oJiQ8PNm6X"},"source":["!pip install transformers"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Bc_MRRv6_CcF"},"source":["青空文庫から夏目漱石の\n","「それから」\n","「こころ」\n","「夢十夜」\n","をダウンロードしてくる"]},{"cell_type":"code","metadata":{"id":"ycNXyyaxOaFH"},"source":["!wget https://www.aozora.gr.jp/cards/000148/files/773_ruby_5968.zip\n","!unzip -O sjjs /content/773_ruby_5968.zip\n","!wget https://www.aozora.gr.jp/cards/000148/files/56143_ruby_50824.zip\n","!unzip -O sjjs  /content/56143_ruby_50824.zip\n","!wget https://www.aozora.gr.jp/cards/000148/files/799_ruby_6024.zip\n","!unzip -O sjjs 799_ruby_6024.zip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BvIa4940dGPX"},"source":["!apt install nkf"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SAWKmgnrdP1s"},"source":["!nkf -w --overwrite kokoro.txt sorekara.txt yume_juya.txt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GDVIiOAMwiaz"},"source":["!cat kokoro.txt sorekara.txt yume_juya.txt > train.txt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5ZlAgWfINnh4"},"source":["from transformers import TFBertModel\n","from transformers import BertJapaneseTokenizer\n","\n","\n","tokenizer = BertJapaneseTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')\n","\n","bert = TFBertModel.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OS5NcVosMcv3"},"source":["import MeCab\n","import numpy as np\n","import tensorflow as tf\n","import os"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dlHfGEMfvzjE"},"source":["with open('train.txt', 'r', encoding='utf-8') as f:\n","  text = f.read().replace('\\n', '')\n","mecab = MeCab.Tagger(\"-Owakati\")\n","text = mecab.parse(text).split()\n","vocab = sorted(set(text))\n","char2idx = {u: i for i, u in enumerate(vocab)}\n","idx2char = np.array(vocab)\n","text_as_int = np.array([char2idx[c] for c in text])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2LadQxfjxZJK"},"source":["seq_length = 128\n","\n","# 訓練用サンプルとターゲットを作る\n","char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n","sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n","def split_input_target(chunk):\n","    input_text = chunk[:-1]\n","    target_text = chunk[1:]\n","    return input_text, target_text\n","\n","dataset = sequences.map(split_input_target)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jgXj3CNpphYp"},"source":["for input_example, target_example in dataset.take(3):\n","    print(f'Input data: {repr(\"\".join(idx2char[input_example.numpy()]))}')\n","    print(f'Target data: {repr(\"\".join(idx2char[target_example.numpy()]))}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WP8h98aFTjOS"},"source":["ラベルのサイズ(バッチサイズ、　文の長さ)\n","\n","出力のサイズ（バッチサイズ、　文の長さ、　ボキャブラリーサイズ）"]},{"cell_type":"code","metadata":{"id":"THKXvUiAd-sz"},"source":["BATCH_SIZE = 64\n","\n","\n","BUFFER_SIZE = 10000\n","\n","dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n","\n","input_ids = tf.keras.layers.Input(shape=(None, ), dtype='int32', name='input_ids')\n","inputs = [input_ids]\n","\n","bert.trainable = False\n","x = bert(inputs)\n","\n","out = x[0]\n","\n","Y = tf.keras.layers.Dense(len(vocab))(out)\n","\n","checkpoint_dir = './training_checkpoints'\n","checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n","\n","checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n","    filepath=checkpoint_prefix,\n","    save_weights_only=True)\n","\n","model = tf.keras.Model(inputs=inputs, outputs=Y)\n","def loss(labels, logits):\n","  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n","\n","model.compile(loss=loss,\n","              optimizer=tf.keras.optimizers.Adam(1e-7))\n","\n","model.fit(dataset,epochs=5, callbacks=[checkpoint_callback])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XiUp6xVb1f9x"},"source":["def generate_text(model, start_string):\n","  # 評価ステップ（学習済みモデルを使ったテキスト生成）\n","\n","  # 生成する文字数\n","  num_generate = 30\n","\n","  # 開始文字列を数値に変換（ベクトル化）\n","  input_eval = [char2idx[s] for s in start_string]\n","  input_eval = tf.expand_dims(input_eval, 0)\n","\n","  # 結果を保存する空文字列\n","  text_generated = []\n","\n","  # 低い temperature　は、より予測しやすいテキストをもたらし\n","  # 高い temperature は、より意外なテキストをもたらす\n","  # 実験により最適な設定を見つけること\n","  temperature = 1\n","\n","  # ここではバッチサイズ　== 1\n","  model.reset_states()\n","  for i in range(num_generate):\n","      predictions = model(input_eval)\n","      # バッチの次元を削除\n","      predictions = tf.squeeze(predictions, 0)\n","\n","      # カテゴリー分布をつかってモデルから返された言葉を予測 \n","      predictions = predictions / temperature\n","      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n","\n","      # 過去の隠れ状態とともに予測された言葉をモデルへのつぎの入力として渡す\n","      input_eval = tf.expand_dims([predicted_id], 0)\n","\n","      text_generated.append(idx2char[predicted_id])\n","\n","  return (''.join(start_string) + ''.join(text_generated))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y8kz6emY9erh"},"source":["text = '私は'\n","mecab = MeCab.Tagger(\"-Owakati\")\n","text = mecab.parse(text).split()\n","generate_text(model, text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AoNYqcNwvdg7"},"source":["import tensorflow as tf"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1n1haJHIjkMC"},"source":["def input_target(chunk):\n","    input_text = chunk\n","    target = tf.constant([1, 0, 0], dtype=tf.float32)\n","    return input_text, target\n","\n","kokoro = tf.data.TextLineDataset('kokoro.txt')\n","kokoro = kokoro.map(input_target)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SfiNaxf_kbdm"},"source":["def input_target(chunk):\n","    input_text = chunk\n","    target = tf.constant([0, 1, 0], dtype=tf.float32)\n","    return input_text, target\n","\n","sorekara = tf.data.TextLineDataset('sorekara.txt')\n","sorekara = sorekara.map(input_target)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y7rha8H8kuvI"},"source":["def input_target(chunk):\n","    input_text = chunk\n","    target = tf.constant([0, 0, 1], dtype=tf.float32)\n","    return input_text, target\n","\n","yume_juya = tf.data.TextLineDataset('yume_juya.txt')\n","yume_juya = yume_juya.map(input_target)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GYpXS3aSk2Qd"},"source":["train_dataset = kokoro.concatenate(sorekara).concatenate(yume_juya)\n","\n","def tokenize_map_fn(tokenizer):\n","\n","    \"\"\"map function for pretrained tokenizer\"\"\"\n","    def _tokenize(text_a, label):\n","        inputs = tokenizer.encode_plus(\n","            text_a.numpy().decode('utf-8'),\n","            add_special_tokens=True,\n","        )\n","        input_ids= inputs[\"input_ids\"]\n","        return input_ids, label\n","\n","    def _map_fn(text,label):\n","        out = tf.py_function(_tokenize, inp=[text, label], Tout=(tf.int32, tf.float32))\n","        return (out[0], out[1])\n","\n","    return _map_fn\n","\n","\n","train_dataset = train_dataset.map(tokenize_map_fn(tokenizer))\n","train_dataset = train_dataset.map(lambda x, y : (x[:128], y))\n","train_dataset = train_dataset.padded_batch(64, padded_shapes=([128], [3]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bza1i9kelF1g"},"source":["BUFFER_SIZE = 10000\n","\n","dataset = train_dataset.shuffle(BUFFER_SIZE)\n","\n","input_ids = tf.keras.layers.Input(shape=(None, ), dtype='int32', name='input_ids')\n","inputs = [input_ids]\n","\n","bert.trainable = False\n","x = bert(inputs)\n","\n","out = x[1]\n","\n","fully_connected = tf.keras.layers.Dense(256, activation='relu')(out)\n","Y = tf.keras.layers.Dense(3, activation='softmax')(fully_connected)\n","\n","checkpoint_dir = './training_checkpoints'\n","checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n","\n","checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n","    filepath=checkpoint_prefix,\n","    save_weights_only=True)\n","\n","model = tf.keras.Model(inputs=inputs, outputs=Y)\n","def loss(labels, logits):\n","  return tf.keras.losses.categorical_crossentropy(labels, logits)\n","\n","model.compile(loss=loss,\n","              optimizer=tf.keras.optimizers.Adam(1e-7))\n","\n","model.fit(dataset,epochs=5, callbacks=[checkpoint_callback])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AiLa_AcnlNCl"},"source":["\n","text = '楽しい勉強でした。'\n","\n","\n","encoded = tokenizer.encode_plus(\n","            text,\n","            text,\n","            add_special_tokens=True,\n","            max_length=128,\n","            pad_to_max_length=True,\n","            return_attention_mask=True\n","        )\n","inputs = tf.expand_dims(encoded[\"input_ids\"],0)\n","res = model.predict_on_batch(inputs)\n","res"],"execution_count":null,"outputs":[]}]}