{"nbformat":4,"nbformat_minor":2,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3.7.9 64-bit ('.venv': poetry)"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"},"colab":{"name":"4_1_tensorflow_codes_after_tf2.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"accelerator":"GPU","interpreter":{"hash":"fc4a78780ec9c8f5fd41b12b5329a21350b5c7c69ca5abee6b7cfefb9cdbec51"}},"cells":[{"cell_type":"markdown","source":["# 準備\r\n","\r\n","## Google Colab 用の処理\r\n","\r\n","下記を実行します\r\n","- ドライブのマウント\r\n","- ノートブックファイルと同じフォルダへの移動 \r\n","\r\n","Googleドライブのマイドライブ を基準に DNN_code/DNN_code_colab_day2 フォルダを置くことを仮定しています。必要に応じて，パスを変更してください．"],"metadata":{"id":"MtR9S1aybb27"}},{"cell_type":"code","execution_count":null,"source":["# Google Colab での実行かを調べる\r\n","import sys\r\n","import os\r\n","ENV_COLAB = True  if 'google.colab' in sys.modules else False \r\n","\r\n","# google drive のマウント\r\n","if ENV_COLAB:\r\n","  from google.colab import drive \r\n","  drive.mount('/content/drive')\r\n","  os.chdir('/content/drive/My Drive/DNN_code/DNN_code_colab_day2/notebook')"],"outputs":[],"metadata":{"id":"wONHgg-Rbb3A"}},{"cell_type":"markdown","source":["## sys.pathの設定"],"metadata":{"id":"XVYVAvKEbb3C"}},{"cell_type":"code","execution_count":null,"source":["import sys\r\n","sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定"],"outputs":[],"metadata":{"id":"-FGmCNlabb3D"}},{"cell_type":"markdown","source":["# TensolFlow"],"metadata":{"id":"eKZrXR2_RmT4"}},{"cell_type":"markdown","source":["## base"],"metadata":{"id":"e5UAwu5xRmT6"}},{"cell_type":"markdown","source":["### constant"],"metadata":{"id":"zsrVmuLBRmT7"}},{"cell_type":"code","execution_count":null,"source":["import tensorflow as tf\n","import numpy as np\n","tf.config.run_functions_eagerly(False)\n","\n","# それぞれ定数を定義\n","a = tf.constant(1)\n","b = tf.constant(2, dtype=tf.float32, shape=[3,2])\n","c = tf.constant(np.arange(4), dtype=tf.float32, shape=[2,2])\n","\n","print('a:', a)\n","print('b:', b)\n","print('c:', c)\n"],"outputs":[],"metadata":{"id":"jPyeAp4-RmT8"}},{"cell_type":"markdown","source":["### placeholder\n","\n","placeholder は、tensorflow ver.1 系にて\n","「構築した計算グラフが、\"非定数値\" (ex. モデルの入出力など) をどう扱うか」の情報を与えるために\n","用いられていたが、tensorflow ver.2 系では、ユーザーが明示的に使用することはほぼなくなった。\n","(参考までに tensoflow ver.1 のコードを行かにコメントアウトして残しておく)\n","\n","\n","- 参考:\n","  - [tensorflow.keras.input](https://www.tensorflow.org/api_docs/python/tf/keras/Input) などでは、内部的に placeholder の仕組みが使用されているようである。"],"metadata":{"id":"jPuQJFqyRmT_"}},{"cell_type":"code","execution_count":null,"source":["# import tensorflow as tf\n","# import numpy as np\n","# tf.config.run_functions_eagerly(False)\n","\n","# # プレースホルダーを定義\n","# x = tf.compat.v1.placeholder(dtype=tf.float32, shape=[None,3])\n","\n","# print('x:', x)\n","\n","# sess = tf.compat.v1.Session()\n","\n","# X = np.random.rand(2,3)\n","# print('X:', X)\n","\n","# # プレースホルダにX[0]を入力\n","# # shapeを(3,)から(1,3)にするためreshape\n","# print('x:', sess.run(x, feed_dict={x:X[0].reshape(1,-1)}))\n","# # プレースホルダにX[1]を入力\n","# print('x:', sess.run(x, feed_dict={x:X[1].reshape(1,-1)}))"],"outputs":[],"metadata":{"id":"JAeUprvWRmUA"}},{"cell_type":"markdown","source":["### variables"],"metadata":{"id":"oIWGRXzuRmUD"}},{"cell_type":"code","execution_count":null,"source":["# 定数を定義\n","a = tf.constant(10)\n","print('a:', a)\n","# 変数を定義\n","x = tf.Variable(1)\n","print('x:', x)\n","\n","# x * a の計算グラフを予め定義\n","@tf.function\n","def calc_x_by_a(x,a):\n","    return x * a\n"," \n","print(x.numpy())\n","\n","x = calc_x_by_a(x,a)\n","\n","print(x.numpy())\n","\n","x = calc_x_by_a(x,a)\n","\n","print(x.numpy())\n","\n"],"outputs":[],"metadata":{"id":"TOminliIRmUD"}},{"cell_type":"markdown","source":["## 線形回帰\n","---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n","### [try]\n","-  noiseの値を変更しよう\n","-  dの数値を変更しよう\n","---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"],"metadata":{"id":"nIEyq1hRRmUG"}},{"cell_type":"code","execution_count":null,"source":["import numpy as np\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","tf.config.run_functions_eagerly(False)\n","\n","iters_num = 300\n","plot_interval = 10\n","\n","# データを生成\n","n = 100\n","x = np.random.rand(n).astype(np.float32)\n","d = 3 * x + 2\n","\n","# ノイズを加える\n","noise = 0.3\n","d = d + noise * np.random.randn(n).astype(np.float32)\n","\n","# 最適化の対象の変数を初期化\n","W = tf.Variable(tf.zeros([1]))\n","b = tf.Variable(tf.zeros([1]))\n","trainable_variables= [W,b]\n","\n","@tf.function\n","def linear_regression(xt, W, b):\n","    return W * xt + b\n","\n","@tf.function\n","def calc_loss(y, dt):\n","    return tf.reduce_mean(input_tensor=tf.square(y - dt))\n","\n","# 誤差関数 平均2乗誤差\n","optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)\n","\n","\n","# 作成したデータをトレーニングデータとして準備\n","x_train = x.reshape(-1,1)\n","d_train = d.reshape(-1,1)\n","\n","@tf.function\n","def train_step(inputs, labels):\n","    with tf.GradientTape() as tape:\n","        y = linear_regression(inputs, W, b)\n","        loss = calc_loss(y, labels)\n","    gradients = tape.gradient(loss, trainable_variables) # 勾配 (loss を trainable_variables の各変数で微分) を求める\n","    optimizer.apply_gradients(zip(gradients, trainable_variables)) # 勾配を用いて変数の更新\n","    return loss\n","\n","# %% トレーニング\n","iters_num = 100\n","plot_interval = 10\n","\n","for i in range(iters_num):\n","    loss_val = train_step(x, d)\n","    if (i+1) % plot_interval == 0:\n","        print('Generation: ' + str(i+1) + '. 誤差 = ' + str(loss_val.numpy()))\n","\n","# 学習された係数\n","print(\"W:\", W.numpy())\n","print(\"b:\", b.numpy())\n","\n","\n","# 回帰直線とデータのプロット\n","fig = plt.figure()\n","subplot = fig.add_subplot(1, 1, 1)\n","plt.scatter(x, d)\n","linex = np.linspace(0, 1, 2).astype(np.float32)\n","liney = linear_regression(linex, W, b)\n","subplot.plot(linex,liney)\n","plt.show()"],"outputs":[],"metadata":{"id":"G_PCjKsFRmUH"}},{"cell_type":"markdown","source":["## 非線形回帰\n","---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n","### [try]\n","-  noiseの値を変更しよう\n","-  dの数値を変更しよう\n","---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"],"metadata":{"id":"WqU8Dx_nRmUK"}},{"cell_type":"code","execution_count":null,"source":["import numpy as np\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","tf.config.run_functions_eagerly(False)\n","\n","# データを生成\n","n=100\n","x = np.random.rand(n).astype(np.float32) * 4 - 2\n","d =  - 0.4 * x ** 3 + 1.6 * x ** 2 - 2.8 * x + 1\n","\n","#  ノイズを加える\n","noise = 0.05\n","d = d + noise * np.random.randn(n)\n","\n","# %% \n","# モデル\n","#  y = w0 + w1 * x + w2 * x^2 + w3 * x^3 \n","#  \n","#  note:\n","#  - 重みW については線形のため、モデル入力をベクトル [1, x, x^2, x^3] であるとする\n","#  - 線形回帰のバイアスは、w0 として W に含まれる\n","\n","# numpy での予測関数\n","def predict(x,W_val):\n","    result = 0.\n","    for i in range(0,4):\n","        result += W_val[i,0] * x ** i\n","    return result\n","\n","# 学習のための tensorflow のモデル関数\n","@tf.function\n","def model_function(xt, W):\n","    y=tf.matmul(xt,W)\n","    return y\n","\n","# 誤差関数 平均２乗誤差\n","@tf.function \n","def loss_fn(y,dt):\n","    loss = tf.reduce_mean(input_tensor=tf.square(y - dt))\n","    return loss\n","\n","# 最適化の対象の変数を初期化\n","W = tf.Variable(tf.random.normal([4, 1], stddev=0.01))\n","trainable_variables = [W]\n","\n","# %%  トレーニングループの定義\n","@tf.function\n","def train_step(inputs, labels):\n","    with tf.GradientTape() as tape:\n","        y = model_function(inputs, W)\n","        loss = loss_fn(y, labels)\n","    \n","    gradients = tape.gradient(loss, trainable_variables)\n","    optimizer.apply_gradients(zip(gradients, trainable_variables))\n","    return loss\n","\n","# %% \n","optimizer = tf.optimizers.Adam(learning_rate=0.001)\n","\n","# %%  トレーニングの実行\n","d_train = d.reshape(-1,1).astype(np.float32)\n","x_train = np.zeros([n, 4]).astype(np.float32)\n","for i in range(n):\n","    for j in range(4):\n","        x_train[i, j] = x[i]**j\n","\n","#  トレーニング\n","iters_num = 10000\n","plot_interval = 100\n","\n","for i in range(iters_num):\n","    loss_val = train_step(x_train, d_train)\n","    if (i+1) % plot_interval == 0:\n","        print('Generation: ' + str(i+1) + '. 誤差 = ' + str(loss_val.numpy()))\n","\n","# 学習された係数\n","W_val = W.numpy() \n","print(\"W:\", W_val)\n","\n","# 回帰直線とデータのプロット\n","fig = plt.figure()\n","subplot = fig.add_subplot(1, 1, 1)\n","plt.scatter(x, d)\n","linex = np.linspace(-2, 2, 100).astype(np.float32)\n","liney = predict(linex, W_val)\n","subplot.plot(linex,liney)\n","plt.show()"],"outputs":[],"metadata":{"id":"fEtR0t68RmUK"}},{"cell_type":"markdown","source":["---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n","## [try]\n","-  次の式をモデルとして回帰を行おう\n","$$ y=30x^{2} +0.5x+0.2 $$<br>\n","-  誤差が収束するようiters_numやlearning_rateを調整しよう\n","---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"],"metadata":{"id":"UIehwtYLRmUN"}},{"cell_type":"code","execution_count":null,"source":["import numpy as np\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","tf.config.run_functions_eagerly(False)\n","\n","iters_num = 30000\n","plot_interval = 1000\n","\n","input_layer_size = 3\n","output_layer_size = 1\n","\n","# データを生成\n","n=100\n","x = np.random.rand(n).astype(np.float32) * 4 - 2\n","d = 30. * x ** 2 + 0.5 * x + 0.2\n","\n","#  ノイズを加える\n","# noise = 0.05\n","# d = d + noise * np.random.randn(n)\n","\n","# %% \n","# モデル\n","#  y = \\sigma_{i=0}^{N} ( w_i * x^i )  \n","#  \n","#  note:\n","#  - N = input_layer_size\n","#  - 重みW については線形のため、モデル入力をベクトル [1, x, x^2, x^3] であるとする\n","#  - 線形回帰のバイアスは、w0 として W に含まれる\n","\n","# 予測関数\n","def predict(x):\n","    result = 0.\n","    for i in range(0,input_layer_size):\n","        result += W_val[i,0] * x ** i\n","    return result\n","\n","# 学習のための tensorflow のモデル関数\n","@tf.function\n","def model_function(xt, W):\n","    y=tf.matmul(xt,W)\n","    return y\n","\n","# 誤差関数 平均２乗誤差\n","@tf.function \n","def loss_fn(y,dt):\n","    loss = tf.reduce_mean(input_tensor=tf.square(y - dt))\n","    return loss\n","\n","# 最適化の対象の変数を初期化\n","W = tf.Variable(tf.random.normal([input_layer_size, 1], stddev=0.01))\n","trainable_variables = [W]\n","\n","# %%  トレーニングループの定義\n","@tf.function\n","def train_step(inputs, labels):\n","    with tf.GradientTape() as tape:\n","        y = model_function(inputs, W)\n","        loss = loss_fn(y, labels)\n","    \n","    gradients = tape.gradient(loss, trainable_variables)\n","    optimizer.apply_gradients(zip(gradients, trainable_variables))\n","    return loss\n","\n","\n","# 作成したデータをトレーニングデータとして準備\n","d_train = d.reshape([n, output_layer_size]).astype(np.float32)\n","x_train = np.zeros([n, input_layer_size]).astype(np.float32)\n","for i in range(n):\n","    for j in range(input_layer_size):\n","        x_train[i, j] = x[i]**j\n","\n","\n","#  トレーニング\n","# 誤差関数 平均２乗誤差\n","optimizer = tf.optimizers.Adam(0.01)\n","\n","for i in range(iters_num):\n","    loss_val = train_step(x_train, d_train)\n","    if (i+1) % plot_interval == 0:\n","        print('Generation: ' + str(i+1) + '. 誤差 = ' + str(loss_val.numpy()))\n","\n","\n","# 学習された係数\n","W_val = W.numpy() \n","print(\"W:\", W_val[::-1])\n","\n","fig = plt.figure()\n","subplot = fig.add_subplot(1,1,1)\n","plt.scatter(x ,d)\n","linex = np.linspace(-2,2,100)\n","liney = predict(linex)\n","subplot.plot(linex,liney)\n","plt.show()"],"outputs":[],"metadata":{"id":"LkBYx9L0RmUN"}},{"cell_type":"markdown","source":["## 分類1層 (mnist)\n","---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n","## [try]\n","-  x：入力値, d：教師データ, W：重み, b：バイアス をそれぞれ定義しよう\n","---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"],"metadata":{"id":"li1cUcrYRmUQ"}},{"cell_type":"code","execution_count":null,"source":["import tensorflow as tf\n","import numpy as np\n","from sklearn.utils import shuffle\n","import matplotlib.pyplot as plt\n","from data.mnist import load_mnist\n","import logging\n","\n","# ロギングレベルの変更\n","logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n","\n","# 訓練用・テスト用(検証用) データの準備\n","(x_train, d_train), (x_test, d_test) = load_mnist(flatten=True, one_hot_label=True)\n","x_train = x_train.astype(np.float32)\n","d_train = d_train.astype(np.float32)\n","\n","x_test = x_test.astype(np.float32)\n","d_test = d_test.astype(np.float32)\n","\n","# %% モデル定義\n","@tf.function\n","def model_function(x):\n","    y = tf.nn.softmax(tf.matmul(x, W) + b)\n","    return y\n","\n","# 最適化すべきパラメータ\n","W = tf.Variable(tf.random.normal([784, 10], stddev=0.01))\n","b = tf.Variable(tf.zeros([10]))\n","trainable_variables = [W, b]\n","\n","\n","# %% その他 tensorflow を用いて計算する関数の定義\n","# 誤差関数 (交差エントロピーの平均値)\n","@tf.function\n","def loss_fn(y, d):\n","    cross_entropy = -tf.reduce_sum(input_tensor=d * tf.math.log(y), axis=[1])\n","    loss = tf.reduce_mean(input_tensor=cross_entropy)\n","    return loss\n","\n","# 正解率\n","@tf.function\n","def accuracy(y,d):\n","    correct = tf.equal(tf.argmax(input=y, axis=1), tf.argmax(input=d, axis=1))\n","    acc = tf.reduce_mean(tf.cast(correct, tf.float32))\n","    return acc\n","\n","\n","\n","# 最適化手法は SGDを用いる\n","optimizer = tf.optimizers.SGD(learning_rate=0.1)\n","\n","# %%  トレーニングループの定義\n","@tf.function\n","def train_step(inputs, labels):\n","    with tf.GradientTape() as tape:\n","        loss = loss_fn(model_function(inputs), labels)\n","    gradients = tape.gradient(loss, trainable_variables)\n","    optimizer.apply_gradients(zip(gradients, trainable_variables))\n","    return loss\n","\n","# %% 学習ループ\n","iters_num = 100\n","batch_size = 100\n","plot_interval = 1\n","\n","accuracies = []\n","iters_per_epoch = len(x_train) // batch_size  \n","for i in range(iters_num):\n","    \n","    # epoch の最初でデータをシャッフルし、ミニバッチ取り出しの カウンターをリセット\n","    if i % iters_per_epoch == 0:\n","        x_train, d_train = shuffle(x_train, d_train, random_state=0)\n","        batch_count = 0\n","\n","    # ミニバッチ取り出し\n","    start_ind = batch_size * batch_count \n","    end_ind   = start_ind + batch_size\n","    x_batch = tf.Variable(x_train[start_ind:end_ind])\n","    d_batch = tf.Variable(d_train[start_ind:end_ind])\n","    batch_count +=1\n","\n","    # 係数更新\n","    loss = train_step(x_batch, d_batch)\n","\n","    # 1 回更新する毎に検証データに対する精度を計算\n","    if (i+1) % plot_interval == 0:\n","        # validation\n","        accuracy_val = accuracy(model_function( tf.Variable(x_test)), tf.Variable(d_test) ).numpy()\n","        accuracies.append(accuracy_val)\n","        print('Generation: ' + str(i+1) + '. 正解率 = ' + str(accuracy_val))\n","\n","\n","# 学習中の正答率の推移を表示\n","lists = range(0, iters_num, plot_interval)\n","plt.plot(lists, accuracies)\n","plt.title(\"accuracy\")\n","plt.ylim(0, 1.0)\n","plt.show()"],"outputs":[],"metadata":{"id":"_cpOZVaiRmUR"}},{"cell_type":"markdown","source":["## 分類3層 (mnist)\n","\n","\n","---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n","## [try]\n","-  隠れ層のサイズを変更してみよう\n","-  optimizerを変更しよう<br>\n","\n","tf.optimizers.SGD<br>\n","`__init__(\n","    learning_rate=0.01, momentum=0.0, nesterov=False, name='SGD', **kwargs\n",")`\n","\n","※ SGDはモメンタムの機能も兼ねる\n","\n","tf.optimizers.Adagrad<br>\n","`__init__(\n","    learning_rate=0.001, initial_accumulator_value=0.1, epsilon=1e-07,\n","    name='Adagrad', **kwargs\n",")`\n","\n","tf.optimizers.RMSPropOptimizer<br>\n","`__init__(\n","    learning_rate=0.001, \n","    rho=0.9, \n","    momentum=0.0, \n","    epsilon=1e-07, \n","    centered=False,\n","    name='RMSprop', \n","    **kwargs\n",")`\n","\n","tf.train.AdamOptimizer<br>\n","`__init__(\n","    learning_rate=0.001, \n","    beta_1=0.9, \n","    beta_2=0.999, \n","    epsilon=1e-07,\n","    amsgrad=False,\n","    name='Adam',\n","     **kwargs\n",")`\n","\n","---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"],"metadata":{"id":"TfzuyAcRRmUU"}},{"cell_type":"code","execution_count":null,"source":["import tensorflow as tf\n","import numpy as np\n","from sklearn.utils import shuffle\n","import matplotlib.pyplot as plt\n","from data.mnist import load_mnist\n","import logging\n","\n","# ロギングレベルの変更\n","logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n","\n","# 訓練用・テスト用(検証用) データの準備\n","(x_train, d_train), (x_test, d_test) = load_mnist(flatten=True, one_hot_label=True)\n","x_train = x_train.astype(np.float32)\n","d_train = d_train.astype(np.float32)\n","\n","x_test = x_test.astype(np.float32)\n","d_test = d_test.astype(np.float32)\n","\n","# %% モデル定義\n","# 最適化すべきパラメータ\n","hidden_layer_size_1 = 600\n","hidden_layer_size_2 = 300\n","dropout_rate = 0.5\n","\n","W1 = tf.Variable(tf.random.normal([784, hidden_layer_size_1], stddev=0.01))\n","W2 = tf.Variable(tf.random.normal([hidden_layer_size_1, hidden_layer_size_2], stddev=0.01))\n","W3 = tf.Variable(tf.random.normal([hidden_layer_size_2, 10], stddev=0.01))\n","b1 = tf.Variable(tf.zeros([hidden_layer_size_1]))\n","b2 = tf.Variable(tf.zeros([hidden_layer_size_2]))\n","b3 = tf.Variable(tf.zeros([10]))\n","trainable_variables = [W1, b1, W2, b2, W3, b3]\n","\n","# 推論関数\n","@tf.function\n","def model_function(x, keep_prob):\n","    z1 = tf.sigmoid(tf.matmul(x, W1) + b1)\n","    z2 = tf.sigmoid(tf.matmul(z1, W2) + b2)\n","    drop = tf.nn.dropout(z2, rate=1 - (keep_prob))\n","    y = tf.nn.softmax(tf.matmul(drop, W3) + b3)\n","    return y\n","\n","# %% その他 tensorflow を用いた計算の定義\n","\n","# 誤差関数 (交差エントロピーの平均値)\n","@tf.function\n","def loss_fn(y, d):\n","    cross_entropy = -tf.reduce_sum(input_tensor=d * tf.math.log(y), axis=[1])\n","    loss = tf.reduce_mean(input_tensor=cross_entropy)\n","    return loss\n","\n","# 正解率\n","@tf.function\n","def accuracy(y,d):\n","    correct = tf.equal(tf.argmax(input=y, axis=1), tf.argmax(input=d, axis=1))\n","    acc = tf.reduce_mean(tf.cast(correct, tf.float32))\n","    return acc\n","\n","# 最適化手法\n","# optimizer = tf.optimizers.SGD(learning_rate=0.5) # 純粋なSGD\n","# optimizer = tf.optimizers.SGD(learning_rate=0.1,momentum=0.9) # モメンタム\n","# optimizer = tf.optimizers.AdaGrad(learning_rate=0.1)\n","# optimizer = tf.optimizers.RMSprop(learning_rate=0.1)\n","optimizer = tf.optimizers.Adam(learning_rate=1e-4)\n","\n","\n","# %%  トレーニングループの定義\n","@tf.function\n","def train_step(inputs, labels, keep_prob):\n","    with tf.GradientTape() as tape:\n","        loss = loss_fn(model_function(inputs, keep_prob), labels)\n","    gradients = tape.gradient(loss, trainable_variables)\n","    optimizer.apply_gradients(zip(gradients, trainable_variables))\n","    return loss\n","\n","# %% 学習ループ\n","iters_num = 3000\n","batch_size = 100\n","plot_interval = 100\n","\n","accuracies = []\n","iters_per_epoch = len(x_train) // batch_size  \n","for i in range(iters_num):\n","    \n","    # epoch の最初でデータをシャッフルし、ミニバッチ取り出しの カウンターをリセット\n","    if i % iters_per_epoch == 0:\n","        x_train, d_train = shuffle(x_train, d_train, random_state=0)\n","        batch_count = 0\n","\n","    # ミニバッチ取り出し\n","    start_ind = batch_size * batch_count \n","    end_ind   = start_ind + batch_size\n","    x_batch = tf.Variable(x_train[start_ind:end_ind])\n","    d_batch = tf.Variable(d_train[start_ind:end_ind])\n","    batch_count +=1\n","\n","    # 係数更新\n","    loss = train_step(x_batch, d_batch, 1-dropout_rate)\n","\n","    # 1 回更新する毎に検証データに対する精度を計算\n","    if (i+1) % plot_interval == 0:\n","        # validation\n","        accuracy_val = accuracy(model_function( tf.Variable(x_test), 1.0 ), tf.Variable(d_test) ).numpy()\n","        accuracies.append(accuracy_val)\n","        print('Generation: ' + str(i+1) + '. 正解率 = ' + str(accuracy_val))\n","\n","\n","# 学習中の正答率の推移を表示\n","lists = range(0, iters_num, plot_interval)\n","plt.plot(lists, accuracies)\n","plt.title(\"accuracy\")\n","plt.ylim(0, 1.0)\n","plt.show()\n"],"outputs":[],"metadata":{"id":"Ss87oFDIRmUV"}},{"cell_type":"markdown","source":["## 分類CNN (mnist)\n","conv - relu - pool - conv - relu - pool - <br>\n","affin - relu - dropout - affin - softmax<br>\n","\n","---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n","### [try]\n","-  ドロップアウト率を0に変更しよう\n","---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"],"metadata":{"id":"4ZlODsBYRmUY"}},{"cell_type":"code","execution_count":null,"source":["import tensorflow as tf\n","import numpy as np\n","from sklearn.utils import shuffle\n","import matplotlib.pyplot as plt\n","from data.mnist import load_mnist\n","import logging\n","\n","# ロギングレベルの変更\n","logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n","\n","# 訓練用・テスト用(検証用) データの準備\n","# 全結合層のモデルと異なりベクトル化しない\n","(x_train, d_train), (x_test, d_test) = load_mnist(flatten=False, one_hot_label=True)\n","x_train = x_train.astype(np.float32)\n","d_train = d_train.astype(np.float32)\n","\n","x_test = x_test.astype(np.float32)\n","d_test = d_test.astype(np.float32)\n","\n","\n","# これいるかどうか検討\n","x_train = np.reshape(x_train, [-1,28,28,1])\n","x_test = np.reshape(x_test, [-1,28,28,1])\n","\n","# %% モデル定義\n","# 最適化すべきパラメータ\n","dropout_rate = 0.5\n","# dropout_rate = 0\n","\n","# 第一層(畳み込み) のweightsとbiasのvariable\n","W_conv1 = tf.Variable(tf.random.truncated_normal([5, 5, 1, 32], stddev=0.1))\n","b_conv1 = tf.Variable(tf.constant(0.1, shape=[32]))\n","\n","# 第二層(畳み込み) \n","W_conv2 = tf.Variable(tf.random.truncated_normal([5, 5, 32, 64], stddev=0.1))\n","b_conv2 = tf.Variable(tf.constant(0.1, shape=[64]))\n","\n","# 第三層(全結合)\n","W_fc1 = tf.Variable(tf.random.truncated_normal([7 * 7 * 64, 1024], stddev=0.1))\n","b_fc1 = tf.Variable(tf.constant(0.1, shape=[1024]))\n","\n","# 第四層(全結合)\n","W_fc2 = tf.Variable(tf.random.truncated_normal([1024, 10], stddev=0.1))\n","b_fc2 = tf.Variable(tf.constant(0.1, shape=[10]))\n","\n","trainable_variables = [W_conv1, b_conv1, W_conv2, b_conv2, W_fc1, b_fc1, W_fc2, b_fc2]\n","\n","# 推論関数\n","@tf.function\n","def model_function(x, keep_prob):\n","    # 第一層のconvolutionalとpool\n","    # strides[0] = strides[3] = 1固定\n","    h_conv1 = tf.nn.relu(tf.nn.conv2d(input=x, filters=W_conv1, strides=[1, 1, 1, 1], padding='SAME') + b_conv1)\n","    # プーリングサイズ n*n にしたい場合 ksize=[1, n, n, 1]\n","    h_pool1 = tf.nn.max_pool2d(input=h_conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n","\n","    # 第二層のconvolutionalとpool\n","    h_conv2 = tf.nn.relu(tf.nn.conv2d(input=h_pool1, filters=W_conv2, strides=[1, 1, 1, 1], padding='SAME') + b_conv2)\n","    h_pool2 = tf.nn.max_pool2d(input=h_conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n","\n","    # 第一層と第二層でreduceされてできた特徴に対してrelu\n","    h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n","    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n","    # Dropout\n","    h_fc1_drop = tf.nn.dropout(h_fc1, rate=1 - (keep_prob))\n","\n","    # 出来上がったものに対してSoftmax\n","    y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n","\n","    return y_conv\n","\n","\n","# %% その他 tensorflow を用いた計算の定義\n","\n","# 誤差関数 (交差エントロピーの平均値)\n","@tf.function\n","def loss_fn(y, d):\n","    cross_entropy = -tf.reduce_sum(input_tensor=d * tf.math.log(y), axis=[1])\n","    loss = tf.reduce_mean(input_tensor=cross_entropy)\n","    return loss\n","\n","# 正解率\n","@tf.function\n","def accuracy(y,d):\n","    correct = tf.equal(tf.argmax(input=y, axis=1), tf.argmax(input=d, axis=1))\n","    acc = tf.reduce_mean(tf.cast(correct, tf.float32))\n","    return acc\n","\n","# 最適化手法\n","# optimizer = tf.optimizers.SGD(learning_rate=0.5) # 純粋なSGD\n","# optimizer = tf.optimizers.SGD(learning_rate=0.1,momentum=0.9) # モメンタム\n","# optimizer = tf.optimizers.AdaGrad(learning_rate=0.1)\n","# optimizer = tf.optimizers.RMSprop(learning_rate=0.1)\n","optimizer = tf.optimizers.Adam(learning_rate=1e-4)\n","\n","\n","# %%  トレーニングループの定義\n","@tf.function\n","def train_step(inputs, labels, keep_prob):\n","    with tf.GradientTape() as tape:\n","        loss = loss_fn(model_function(inputs, keep_prob), labels)\n","    gradients = tape.gradient(loss, trainable_variables)\n","    optimizer.apply_gradients(zip(gradients, trainable_variables))\n","    return loss\n","\n","# %% 学習ループ\n","iters_num = 300\n","batch_size = 100\n","plot_interval = 10\n","\n","accuracies = []\n","iters_per_epoch = len(x_train) // batch_size  \n","for i in range(iters_num):\n","    \n","    # epoch の最初でデータをシャッフルし、ミニバッチ取り出しの カウンターをリセット\n","    if i % iters_per_epoch == 0:\n","        x_train, d_train = shuffle(x_train, d_train, random_state=0)\n","        batch_count = 0\n","\n","    # ミニバッチ取り出し\n","    start_ind = batch_size * batch_count \n","    end_ind   = start_ind + batch_size\n","    x_batch = tf.Variable(x_train[start_ind:end_ind])\n","    d_batch = tf.Variable(d_train[start_ind:end_ind])\n","    batch_count +=1\n","\n","    # 係数更新\n","    loss = train_step(x_batch, d_batch, 1-dropout_rate)\n","\n","    # 1 回更新する毎に検証データに対する精度を計算\n","    if (i+1) % plot_interval == 0:\n","        # validation\n","        accuracy_val = accuracy(model_function( tf.Variable(x_test), 1.0 ), tf.Variable(d_test) ).numpy()\n","        accuracies.append(accuracy_val)\n","        print('Generation: ' + str(i+1) + '. 正解率 = ' + str(accuracy_val))\n","\n","\n","# 学習中の正答率の推移を表示\n","lists = range(0, iters_num, plot_interval)\n","plt.plot(lists, accuracies)\n","plt.title(\"accuracy\")\n","plt.ylim(0, 1.0)\n","plt.show()"],"outputs":[],"metadata":{"id":"wRa3vaWxRmUZ"}}]}