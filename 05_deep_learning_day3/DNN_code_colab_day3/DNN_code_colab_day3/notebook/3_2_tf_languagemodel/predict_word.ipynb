{"nbformat":4,"nbformat_minor":2,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3.7.9 64-bit ('.venv': poetry)"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"},"colab":{"name":"predict_word_tf2.ipynb","provenance":[],"collapsed_sections":[]},"interpreter":{"hash":"fc4a78780ec9c8f5fd41b12b5329a21350b5c7c69ca5abee6b7cfefb9cdbec51"}},"cells":[{"cell_type":"markdown","source":["# 準備"],"metadata":{"id":"8cNl2QA_Rnv5"}},{"cell_type":"markdown","source":["## Google Colab 用の処理\r\n","\r\n","下記を実行します\r\n","- ドライブのマウント\r\n","- ノートブックファイルと同じフォルダへの移動 \r\n","\r\n","Googleドライブのマイドライブ を基準に DNN_code/DNN_code_colab_day3 フォルダを置くことを仮定しています。必要に応じて，パスを変更してください．"],"metadata":{"id":"YkwjN1jNVAYy"}},{"cell_type":"code","execution_count":null,"source":["# Google Colab での実行かを調べる\r\n","import sys\r\n","import os\r\n","ENV_COLAB = True  if 'google.colab' in sys.modules else False \r\n","\r\n","# google drive のマウント\r\n","if ENV_COLAB:\r\n","  from google.colab import drive \r\n","  drive.mount('/content/drive')\r\n","  os.chdir('/content/drive/My Drive/DNN_code/DNN_code_colab_day3/notebook/3_2_tf_languagemodel')"],"outputs":[],"metadata":{"id":"pvFXpiH3EVC1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1644310696835,"user_tz":-540,"elapsed":24451,"user":{"displayName":"Kyosuke Matsumoto","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj63g2zqNXoi0FwvAjiGO25LGRlZqCYwHWAKoNWhX4rlul1s_LNGLAWtA-cvIxOmcy99rNCSAXd7f8t7mQfAZMcILDI0FfBt2NfbUEPPGPEJBmoqRjKbXpFVXV6W6OZ3yn2GQSe03gCotzCCCXF5R-glxw-JE5HihdB7VwomM9z9Ub6Vbwhnn7_ARnCc8qYKc6G2y8Co70BgwsFvsQgWci2_6o_qzW7ydtOFS1TrMbS15G49ZXvB0cFrL0gxY4botnU1KGNCnkrX5ZwhjbovIWkge4ggU17MhypUukCfaHl9_KVPO2DmLQIjYCooqPwKqAs5UkvkQpsz90qjzIflwcuNBcHQWxWs_RU1xKTYzmyXtecmVPaa1rm-rk5ReujCChEscbnfwflkczVQ6DWJgqjb_PpCNfzHq1vOLmo60GsytA-AeyPEy3vnRt9ihnFgsUx-0_9t2I9GH7OqRliz-QHADdmnXPPrIixT0-3AtngiR21hpxK3v3LIA8M1Mi-cF0U0Iwxqr0amhoPIVET6o9aa1ff8j5zsnWPM3PCf1d9xzoUo7wc5tNR3sTwkTumL9evkf2_H99wLDTFTyOnMPwHYb5nOIRICwGhPPyZKX_M4xMiSD1vRWqAIxv4VT8w-4I3SoYjR3muZ5eP4xy8edSt6h-h9MkqYFzJEWsweCHCHVqWugqB0EB0OKiANAJ_Hv-BnS7vjhK0ZwCN40MajcEnVdPurcI1yL4b7CkjrlTuVkelrl9rMvhL2kc8pKKxthWEhQ=s64","userId":"01962885165714747888"}},"outputId":"c3848833-7f80-4668-df32-c5592f92f976"}},{"cell_type":"markdown","source":["# predict_word"],"metadata":{"id":"tmFoITp02_Gv"}},{"cell_type":"code","execution_count":null,"source":["import tensorflow as tf\r\n","import logging\r\n","logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\r\n","\r\n","tf.config.run_functions_eagerly(False)"],"outputs":[],"metadata":{"id":"5WePOZ8UjWbu","executionInfo":{"status":"ok","timestamp":1644310713740,"user_tz":-540,"elapsed":3939,"user":{"displayName":"Kyosuke Matsumoto","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj63g2zqNXoi0FwvAjiGO25LGRlZqCYwHWAKoNWhX4rlul1s_LNGLAWtA-cvIxOmcy99rNCSAXd7f8t7mQfAZMcILDI0FfBt2NfbUEPPGPEJBmoqRjKbXpFVXV6W6OZ3yn2GQSe03gCotzCCCXF5R-glxw-JE5HihdB7VwomM9z9Ub6Vbwhnn7_ARnCc8qYKc6G2y8Co70BgwsFvsQgWci2_6o_qzW7ydtOFS1TrMbS15G49ZXvB0cFrL0gxY4botnU1KGNCnkrX5ZwhjbovIWkge4ggU17MhypUukCfaHl9_KVPO2DmLQIjYCooqPwKqAs5UkvkQpsz90qjzIflwcuNBcHQWxWs_RU1xKTYzmyXtecmVPaa1rm-rk5ReujCChEscbnfwflkczVQ6DWJgqjb_PpCNfzHq1vOLmo60GsytA-AeyPEy3vnRt9ihnFgsUx-0_9t2I9GH7OqRliz-QHADdmnXPPrIixT0-3AtngiR21hpxK3v3LIA8M1Mi-cF0U0Iwxqr0amhoPIVET6o9aa1ff8j5zsnWPM3PCf1d9xzoUo7wc5tNR3sTwkTumL9evkf2_H99wLDTFTyOnMPwHYb5nOIRICwGhPPyZKX_M4xMiSD1vRWqAIxv4VT8w-4I3SoYjR3muZ5eP4xy8edSt6h-h9MkqYFzJEWsweCHCHVqWugqB0EB0OKiANAJ_Hv-BnS7vjhK0ZwCN40MajcEnVdPurcI1yL4b7CkjrlTuVkelrl9rMvhL2kc8pKKxthWEhQ=s64","userId":"01962885165714747888"}}}},{"cell_type":"code","execution_count":null,"source":["import numpy as np\r\n","import re\r\n","import glob\r\n","import collections\r\n","import pickle\r\n","\r\n","class Corpus:\r\n","    def __init__(self):\r\n","        self.unknown_word_symbol = \"<???>\" # 出現回数の少ない単語は未知語として定義しておく\r\n","        self.unknown_word_threshold = 3 # 未知語と定義する単語の出現回数の閾値\r\n","        self.corpus_file = \"./corpus/**/*.txt\"\r\n","        self.corpus_encoding = \"utf-8\"\r\n","        self.dictionary_filename = \"./data_for_predict/word_dict.dic\"\r\n","        self.chunk_size = 5\r\n","        self.load_dict()\r\n","        words = []\r\n","        for filename in glob.glob(self.corpus_file, recursive=True):\r\n","            with open(filename, \"r\", encoding=self.corpus_encoding) as f:\r\n","                # word breaking\r\n","                text = f.read()\r\n","                # 全ての文字を小文字に統一し、改行をスペースに変換\r\n","                text = text.lower().replace(\"\\n\", \" \")\r\n","                # 特定の文字以外の文字を空文字に置換する\r\n","                text = re.sub(r\"[^a-z '\\-]\", \"\", text)\r\n","                # 複数のスペースはスペース一文字に変換\r\n","                text = re.sub(r\"[ ]+\", \" \", text)\r\n","\r\n","                # 前処理： '-' で始まる単語は無視する\r\n","                words = [ word for word in text.split() if not word.startswith(\"-\")]\r\n","\r\n","        self.data_n = len(words) - self.chunk_size\r\n","        self.data = self.seq_to_matrix(words)\r\n","\r\n","    def prepare_data(self):\r\n","        \"\"\"\r\n","        訓練データとテストデータを準備する。\r\n","        data_n = ( text データの総単語数 ) - chunk_size\r\n","        input: (data_n, chunk_size, vocabulary_size)\r\n","        output: (data_n, vocabulary_size)\r\n","        \"\"\"\r\n","\r\n","        # 入力と出力の次元テンソルを準備\r\n","        all_input = np.zeros([self.chunk_size, self.vocabulary_size, self.data_n])\r\n","        all_output = np.zeros([self.vocabulary_size, self.data_n])\r\n","\r\n","        # 準備したテンソルに、コーパスの one-hot 表現(self.data) のデータを埋めていく\r\n","        # i 番目から ( i + chunk_size - 1 ) 番目までの単語が１組の入力となる\r\n","        # このときの出力は ( i + chunk_size ) 番目の単語\r\n","        for i in range(self.data_n):\r\n","            all_output[:, i] = self.data[:, i + self.chunk_size] # (i + chunk_size) 番目の単語の one-hot ベクトル\r\n","            for j in range(self.chunk_size):\r\n","                all_input[j, :, i] = self.data[:, i + self.chunk_size - j - 1]\r\n","\r\n","        # 後に使うデータ形式に合わせるために転置を取る\r\n","        all_input = all_input.transpose([2, 0, 1])\r\n","        all_output = all_output.transpose()\r\n","\r\n","        # 訓練データ：テストデータを 4 : 1 に分割する\r\n","        training_num = ( self.data_n * 4 ) // 5\r\n","        return all_input[:training_num], all_output[:training_num], all_input[training_num:], all_output[training_num:]\r\n","\r\n","\r\n","    def build_dict(self):\r\n","        # コーパス全体を見て、単語の出現回数をカウントする\r\n","        counter = collections.Counter()\r\n","        for filename in glob.glob(self.corpus_file, recursive=True):\r\n","            with open(filename, \"r\", encoding=self.corpus_encoding) as f:\r\n","\r\n","                # word breaking\r\n","                text = f.read()\r\n","                # 全ての文字を小文字に統一し、改行をスペースに変換\r\n","                text = text.lower().replace(\"\\n\", \" \")\r\n","                # 特定の文字以外の文字を空文字に置換する\r\n","                text = re.sub(r\"[^a-z '\\-]\", \"\", text)\r\n","                # 複数のスペースはスペース一文字に変換\r\n","                text = re.sub(r\"[ ]+\", \" \", text)\r\n","\r\n","                # 前処理： '-' で始まる単語は無視する\r\n","                words = [word for word in text.split() if not word.startswith(\"-\")]\r\n","\r\n","                counter.update(words)\r\n","\r\n","        # 出現頻度の低い単語を一つの記号にまとめる\r\n","        word_id = 0\r\n","        dictionary = {}\r\n","        for word, count in counter.items():\r\n","            if count <= self.unknown_word_threshold:\r\n","                continue\r\n","\r\n","            dictionary[word] = word_id\r\n","            word_id += 1\r\n","        dictionary[self.unknown_word_symbol] = word_id\r\n","\r\n","        print(\"総単語数：\", len(dictionary))\r\n","\r\n","        # 辞書を pickle を使って保存しておく\r\n","        with open(self.dictionary_filename, \"wb\") as f:\r\n","            pickle.dump(dictionary, f)\r\n","            print(\"Dictionary is saved to\", self.dictionary_filename)\r\n","\r\n","        self.dictionary = dictionary\r\n","        print(self.dictionary)\r\n","\r\n","    def load_dict(self):\r\n","        with open(self.dictionary_filename, \"rb\") as f:\r\n","            self.dictionary = pickle.load(f)\r\n","            self.vocabulary_size = len(self.dictionary)\r\n","            self.input_layer_size = len(self.dictionary)\r\n","            self.output_layer_size = len(self.dictionary)\r\n","            print(\"総単語数: \", self.input_layer_size)\r\n","\r\n","    def get_word_id(self, word):\r\n","        # print(word)\r\n","        # print(self.dictionary)\r\n","        # print(self.unknown_word_symbol)\r\n","        # print(self.dictionary[self.unknown_word_symbol])\r\n","        # print(self.dictionary.get(word, self.dictionary[self.unknown_word_symbol]))\r\n","        return self.dictionary.get(word, self.dictionary[self.unknown_word_symbol])\r\n","\r\n","    # 入力された単語を one-hot ベクトルにする\r\n","    def to_one_hot(self, word):\r\n","        index = self.get_word_id(word)\r\n","        data = np.zeros(self.vocabulary_size)\r\n","        data[index] = 1\r\n","        return data\r\n","\r\n","    def seq_to_matrix(self, seq):\r\n","        # print(seq)\r\n","        data = np.array([self.to_one_hot(word) for word in seq]) # (data_n, vocabulary_size)\r\n","        return data.transpose() # (vocabulary_size, data_n)\r\n","\r\n","def build_dict():\r\n","    cp = Corpus()\r\n","    cp.build_dict()\r\n"],"outputs":[],"metadata":{"id":"zdJ2FyJWjWbu","executionInfo":{"status":"ok","timestamp":1644310714117,"user_tz":-540,"elapsed":387,"user":{"displayName":"Kyosuke Matsumoto","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj63g2zqNXoi0FwvAjiGO25LGRlZqCYwHWAKoNWhX4rlul1s_LNGLAWtA-cvIxOmcy99rNCSAXd7f8t7mQfAZMcILDI0FfBt2NfbUEPPGPEJBmoqRjKbXpFVXV6W6OZ3yn2GQSe03gCotzCCCXF5R-glxw-JE5HihdB7VwomM9z9Ub6Vbwhnn7_ARnCc8qYKc6G2y8Co70BgwsFvsQgWci2_6o_qzW7ydtOFS1TrMbS15G49ZXvB0cFrL0gxY4botnU1KGNCnkrX5ZwhjbovIWkge4ggU17MhypUukCfaHl9_KVPO2DmLQIjYCooqPwKqAs5UkvkQpsz90qjzIflwcuNBcHQWxWs_RU1xKTYzmyXtecmVPaa1rm-rk5ReujCChEscbnfwflkczVQ6DWJgqjb_PpCNfzHq1vOLmo60GsytA-AeyPEy3vnRt9ihnFgsUx-0_9t2I9GH7OqRliz-QHADdmnXPPrIixT0-3AtngiR21hpxK3v3LIA8M1Mi-cF0U0Iwxqr0amhoPIVET6o9aa1ff8j5zsnWPM3PCf1d9xzoUo7wc5tNR3sTwkTumL9evkf2_H99wLDTFTyOnMPwHYb5nOIRICwGhPPyZKX_M4xMiSD1vRWqAIxv4VT8w-4I3SoYjR3muZ5eP4xy8edSt6h-h9MkqYFzJEWsweCHCHVqWugqB0EB0OKiANAJ_Hv-BnS7vjhK0ZwCN40MajcEnVdPurcI1yL4b7CkjrlTuVkelrl9rMvhL2kc8pKKxthWEhQ=s64","userId":"01962885165714747888"}}}},{"cell_type":"code","execution_count":null,"source":["import time\r\n","import datetime\r\n","\r\n","class Language:\r\n","    \"\"\"\r\n","    input layer: self.vocabulary_size\r\n","    hidden layer: rnn_size = 30\r\n","    output layer: self.vocabulary_size\r\n","    \"\"\"\r\n","\r\n","    def __init__(self):\r\n","        self.corpus = Corpus()\r\n","        self.dictionary = self.corpus.dictionary\r\n","        self.vocabulary_size = len(self.dictionary) # 単語数\r\n","        self.input_layer_size = self.vocabulary_size # 入力層の数\r\n","        self.hidden_layer_size = 30 # 隠れ層の RNN ユニットの数\r\n","        self.output_layer_size = self.vocabulary_size # 出力層の数\r\n","        self.batch_size = 128 # バッチサイズ\r\n","        self.chunk_size = 5 # 展開するシーケンスの数。c_0, c_1, ..., c_(chunk_size - 1) を入力し、c_(chunk_size) 番目の単語の確率が出力される。\r\n","        self.learning_rate = 0.001 # 学習率\r\n","        self.epochs = 50 # 学習するエポック数\r\n","        self.forget_bias = 1.0 # LSTM における忘却ゲートのバイアス\r\n","        self.model_filename = \"./data_for_predict/predict_model.ckpt\"\r\n","        self.unknown_word_symbol = self.corpus.unknown_word_symbol\r\n","\r\n","        # RNN 入力前の Embedding のパラメータ　\r\n","        self.hidden_w = tf.Variable(tf.random.truncated_normal([self.input_layer_size, self.hidden_layer_size], stddev=0.01))\r\n","        self.hidden_b = tf.Variable(tf.ones([self.hidden_layer_size]))\r\n","\r\n","        # RNN 出力後の 全結合層のパラメータ\r\n","        self.output_w = tf.Variable(tf.random.truncated_normal([self.hidden_layer_size, self.output_layer_size], stddev=0.01))\r\n","        self.output_b = tf.Variable(tf.ones([self.output_layer_size]))\r\n","\r\n","        # RNN \r\n","        #self.rnn = tf.keras.layers.SimpleRNN(self.hidden_layer_size, activation='tanh', return_sequences=True)\r\n","        self.rnn = tf.keras.layers.SimpleRNN(self.hidden_layer_size, activation='tanh')\r\n","        # SimpleRNN Layer の weight を 強制的に生成させる \r\n","        self.rnn(np.zeros((self.chunk_size, self.batch_size, self.hidden_layer_size),np.float32)) \r\n","\r\n","        self.trainable_variables = [self.hidden_w, self.hidden_b, self.output_w, self.output_b, *self.rnn.trainable_variables]\r\n","\r\n","        self.optimizer = None\r\n","\r\n","    def load_weights(self, ckpt_path):\r\n","        ckpt = tf.train.load_checkpoint(ckpt_path)\r\n","\r\n","        # checkpoint から明示的に変数名を指定して保存\r\n","        self.hidden_w=tf.Variable(ckpt.get_tensor(\"hidden_w/.ATTRIBUTES/VARIABLE_VALUE\"))\r\n","        self.hidden_b=tf.Variable(ckpt.get_tensor(\"hidden_b/.ATTRIBUTES/VARIABLE_VALUE\"))\r\n","        self.output_w=tf.Variable(ckpt.get_tensor(\"output_w/.ATTRIBUTES/VARIABLE_VALUE\"))\r\n","        self.output_b=tf.Variable(ckpt.get_tensor(\"output_b/.ATTRIBUTES/VARIABLE_VALUE\"))\r\n","        k1 = tf.Variable(ckpt.get_tensor(\"rnn_kernel/.ATTRIBUTES/VARIABLE_VALUE\"))\r\n","        k2 = tf.Variable(ckpt.get_tensor(\"rnn_reccurent_kernel/.ATTRIBUTES/VARIABLE_VALUE\"))\r\n","        b  = tf.Variable(ckpt.get_tensor(\"rnn_bias/.ATTRIBUTES/VARIABLE_VALUE\"))\r\n","        self.rnn.set_weights([k1,k2,b])\r\n","        return\r\n","    \r\n","    def save_weights(self, model_file):\r\n","        ckpt_tf2 = tf.train.Checkpoint(hidden_w=self.hidden_w, hidden_b=self.hidden_b, \r\n","                               output_w=self.output_w, output_b=self.output_b, \r\n","                               rnn_kernel=self.rnn.weights[0], rnn_reccurent_kernel=self.rnn.weights[1], rnn_bias=self.rnn.weights[2])\r\n","        save_path = ckpt_tf2.save(model_file)\r\n","        print(save_path, \"was saved\")\r\n","        return\r\n","        \r\n","    @tf.function\r\n","    def inference(self, input_data, initial_state):\r\n","        \"\"\"\r\n","        :param input_data: (batch_size, chunk_size, vocabulary_size) 次元のテンソル\r\n","        :param initial_state: (batch_size, hidden_layer_size) 次元の行列\r\n","        :return:\r\n","        \"\"\"\r\n","        batch_size, chunk_size, vocab_size = input_data.shape\r\n","        \r\n","        # 現時点での入力データは (batch_size, chunk_size, input_layer_size) という３次元のテンソル\r\n","        # chunkc_size * batch_size 分の単語に対して一気に 演算を行うため tf.transpose, tf.reshape を駆使してサイズ調整する\r\n","\r\n","        # shape 調整\r\n","        input_data = tf.transpose(a=input_data, perm=[1, 0, 2]) # 転置。(chunk_size, batch_size, vocabulary_size)\r\n","        input_data = tf.reshape(input_data, [-1, self.input_layer_size]) # 変形。(chunk_size * batch_size, input_layer_size)\r\n","        # 単語(シンボル)の ベクトル化\r\n","        input_data = tf.matmul(input_data, self.hidden_w) + self.hidden_b # 重みWとバイアスBを適用。 (chunk_size * batch_size, hidden_layer_size)\r\n","        # shape を 元に戻す\r\n","        input_data = tf.reshape(input_data, [chunk_size, batch_size, self.hidden_layer_size]) # 変形。(chunk_size,  batch_size, hidden_layer_size)\r\n","        input_data = tf.transpose(a=input_data, perm=[1, 0, 2]) # 転置。(batch_size, chunk_size, hidden_layer_size)\r\n","            \r\n","        # RNN の演算 予測が行えればよいので 最後の単語のみ得る\r\n","        output = self.rnn(input_data, initial_state=initial_state)\r\n","        \r\n","        # 最後に隠れ層から出力層につながる重みとバイアスを処理する\r\n","        # 最終的に softmax 関数で処理し、確率として解釈される。\r\n","        # softmax 関数はこの関数の外で定義する。\r\n","        output = tf.matmul(output, self.output_w) + self.output_b\r\n","\r\n","        # # print weights\r\n","        # print(self.hidden_w[0,0]) \r\n","        # print(self.hidden_b[0]) \r\n","        # print(self.output_w[0,0]) \r\n","        # print(self.output_b[0]) \r\n","        # print(self.rnn.weights[0][0,0]) \r\n","        # print(self.rnn.weights[1][0,0]) \r\n","        # print(self.rnn.weights[2][0]) \r\n","                 \r\n","        return output\r\n","\r\n","    def training(self):\r\n","        # 今回は最適化手法として Adam を選択する。\r\n","        # ここの Adam の部分を変えることで、Adagrad, Adadelta などの他の最適化手法を選択することができる\r\n","        optimizer = tf.optimizers.Adam(learning_rate=self.learning_rate)\r\n","        return optimizer\r\n","\r\n","    @tf.function\r\n","    def loss(self, logits, labels):\r\n","        cost = tf.reduce_mean(input_tensor=tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf.stop_gradient(labels)))\r\n","        return cost\r\n","\r\n","    @tf.function\r\n","    def accuracy(self, prediction, labels):\r\n","        correct = tf.equal(tf.argmax(input=prediction, axis=1), tf.argmax(input=labels, axis=1))\r\n","        accuracy = tf.reduce_mean(input_tensor=tf.cast(correct, tf.float32))\r\n","        return accuracy\r\n","\r\n","    @tf.function\r\n","    def train_step(self, inputs, labels, initial_state):\r\n","        with tf.GradientTape() as tape:\r\n","            prediction = self.inference(inputs, initial_state)\r\n","            loss = self.loss(prediction, labels)\r\n","\r\n","        gradients = tape.gradient(loss, self.trainable_variables)\r\n","        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\r\n","        acc  = self.accuracy(prediction, labels)\r\n","        return loss, acc\r\n","\r\n","    def train(self, model_file):\r\n","        \"\"\"\r\n","        :param save_ckpt: 学習した重み係数を保存する checkpoint の名前\r\n","        :return:\r\n","        \"\"\"\r\n","        # 訓練・テストデータの用意\r\n","        trX, trY, teX, teY = self.corpus.prepare_data()\r\n","        training_num = trX.shape[0]\r\n","\r\n","        # ここから実際に学習を走らせる\r\n","        # エポックを回す\r\n","        log_train_acc = []\r\n","        log_train_loss = []\r\n","        # log_val_acc = []\r\n","        # log_val_loss = [] \r\n","        self.optimizer = self.training()\r\n","        for epoch in range(self.epochs):\r\n","            step = 0\r\n","            epoch_loss = 0\r\n","            epoch_acc = 0\r\n","\r\n","            # 訓練データをバッチサイズごとに分けて学習させる (= optimizer を走らせる)\r\n","            # エポックごとの損失関数の合計値や（訓練データに対する）精度も計算しておく\r\n","            while (step + 1) * self.batch_size < training_num:\r\n","                start_idx = step * self.batch_size\r\n","                end_idx = (step + 1) * self.batch_size\r\n","\r\n","                batch_xs = tf.Variable(trX[start_idx:end_idx, :, :].astype(np.float32))\r\n","                batch_ys = tf.Variable(trY[start_idx:end_idx, :].astype(np.float32))\r\n","                initial_state = tf.Variable(np.zeros([self.batch_size, self.hidden_layer_size],dtype=np.float32))\r\n","                c, a = self.train_step(batch_xs, batch_ys, initial_state)\r\n","                # print(\"Epoch:\", epoch, \", step:\", step, \"-- loss:\", c, \" -- accuracy:\", a)\r\n","                epoch_loss += c\r\n","                epoch_acc += a\r\n","                step += 1\r\n","            # コンソールに損失関数の値や精度を出力しておく\r\n","            print(\"Epoch\", epoch, \"completed ouf of\", self.epochs, \"-- loss:\", epoch_loss/step, \" -- accuracy:\",\r\n","                    epoch_acc / step)\r\n","            log_train_acc.append( (epoch_acc / step).numpy())\r\n","            log_train_loss.append((epoch_loss/step ).numpy() )\r\n","            \r\n","        # 最後にテストデータでの精度を計算して表示する\r\n","        inputs = tf.Variable(teX.astype(np.float32))\r\n","        initial_state = tf.Variable(np.zeros([teX.shape[0], self.hidden_layer_size],dtype=np.float32))\r\n","        labels = tf.Variable(teY.astype(np.float32))\r\n","        prediction = self.inference(inputs,initial_state)\r\n","        a = self.accuracy(prediction, labels)\r\n","        c = self.loss(prediction ,labels)\r\n","        # log_val_acc.append( a.numpy() )\r\n","        # log_val_loss.append( c.numpy() )\r\n","\r\n","        history = {\"train_acc\": log_train_acc, \"train_loss\": log_train_loss\r\n","        #, \"val_acc\":log_val_acc, \"val_loss\":log_val_loss\r\n","        }\r\n","        print(\"Accuracy on test:\", a.numpy())\r\n","        \r\n","        # 学習したモデルも保存しておく\r\n","        self.save_weights(model_file)\r\n","        return history\r\n","    \r\n","    def predict(self, seq):\r\n","        \"\"\"\r\n","        文章を入力したときに次に来る単語を予測する\r\n","        :param seq: 予測したい単語の直前の文字列。chunk_size 以上の単語数が必要。\r\n","        :return: \r\n","        \"\"\"\r\n","        @tf.function\r\n","        def get_predictions(input_data, initial_state):\r\n","            return tf.nn.softmax(self.inference(input_data, initial_state))\r\n","\r\n","        @tf.function\r\n","        def get_predicted_labels(predictions):\r\n","            return tf.argmax(predictions, axis=1)    \r\n","\r\n","        # ----------- 入力データの作成\r\n","        # seq を one-hot 表現に変換する。\r\n","        words = [word for word in seq.split() if not word.startswith(\"-\")]\r\n","        x = np.zeros([1, self.chunk_size, self.input_layer_size], dtype=np.float32)\r\n","        for i in range(self.chunk_size):\r\n","            word = seq[len(words) - self.chunk_size + i]\r\n","            index = self.dictionary.get(word, self.dictionary[self.unknown_word_symbol])\r\n","            x[0][i][index] = 1\r\n","        x = tf.Variable(x)\r\n","        initial_state = tf.Variable(np.zeros((1,self.hidden_layer_size), dtype=np.float32))\r\n","        \r\n","        # ----------- ロードしたモデルを使って各単語の出力確率を計算 (tensorflow による計算)\r\n","        u = get_predictions(x, initial_state)\r\n","         \r\n","        # ----------  結果表示\r\n","        keys = list(self.dictionary.keys())    \r\n","\r\n","        # 各単語の確率の表示\r\n","        display_num = self.vocabulary_size # 10        \r\n","        print(\"各単語の出現確率 (降順)\")\r\n","        sorted_index = np.argsort(-u[0])\r\n","        for i in sorted_index[:display_num]:\r\n","            c = self.unknown_word_symbol if i == (self.vocabulary_size - 1) else keys[i]\r\n","            print(c, \":\", u[0][i].numpy())\r\n","\r\n","        # 最も確率が大きいものを予測結果とする\r\n","        v = get_predicted_labels(u)\r\n","        print()\r\n","        print(\"Prediction:\", seq + \" \" + (\"<???>\" if v[0] == (self.vocabulary_size - 1) else keys[v[0]]))\r\n","\r\n","\r\n","        return\r\n"],"outputs":[],"metadata":{"id":"d9_Wdzdj2Sj-","executionInfo":{"status":"ok","timestamp":1644310714865,"user_tz":-540,"elapsed":762,"user":{"displayName":"Kyosuke Matsumoto","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj63g2zqNXoi0FwvAjiGO25LGRlZqCYwHWAKoNWhX4rlul1s_LNGLAWtA-cvIxOmcy99rNCSAXd7f8t7mQfAZMcILDI0FfBt2NfbUEPPGPEJBmoqRjKbXpFVXV6W6OZ3yn2GQSe03gCotzCCCXF5R-glxw-JE5HihdB7VwomM9z9Ub6Vbwhnn7_ARnCc8qYKc6G2y8Co70BgwsFvsQgWci2_6o_qzW7ydtOFS1TrMbS15G49ZXvB0cFrL0gxY4botnU1KGNCnkrX5ZwhjbovIWkge4ggU17MhypUukCfaHl9_KVPO2DmLQIjYCooqPwKqAs5UkvkQpsz90qjzIflwcuNBcHQWxWs_RU1xKTYzmyXtecmVPaa1rm-rk5ReujCChEscbnfwflkczVQ6DWJgqjb_PpCNfzHq1vOLmo60GsytA-AeyPEy3vnRt9ihnFgsUx-0_9t2I9GH7OqRliz-QHADdmnXPPrIixT0-3AtngiR21hpxK3v3LIA8M1Mi-cF0U0Iwxqr0amhoPIVET6o9aa1ff8j5zsnWPM3PCf1d9xzoUo7wc5tNR3sTwkTumL9evkf2_H99wLDTFTyOnMPwHYb5nOIRICwGhPPyZKX_M4xMiSD1vRWqAIxv4VT8w-4I3SoYjR3muZ5eP4xy8edSt6h-h9MkqYFzJEWsweCHCHVqWugqB0EB0OKiANAJ_Hv-BnS7vjhK0ZwCN40MajcEnVdPurcI1yL4b7CkjrlTuVkelrl9rMvhL2kc8pKKxthWEhQ=s64","userId":"01962885165714747888"}}}},{"cell_type":"markdown","source":["構築済みの辞書と、学習済みのパラメータを用いて予測を行う"],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["ln = Language()\r\n","\r\n","# 学習済みのパラメータをロード\r\n","ln.load_weights(\"./data_for_predict/predict_model\")\r\n","\r\n","# 保存したモデルを使って単語の予測をする\r\n","ln.predict(\"some of them looks like\")"],"outputs":[],"metadata":{"id":"Q2ocSMErjWb6","outputId":"6966afc3-7902-4bbe-9419-f9732dfe45f8"}},{"cell_type":"markdown","source":["学習を試したい場合は、下記をアンコメントして実行してもよい。\r\n","(実行時間が長いため、適宜 エポック数など調整すること)"],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["# # 辞書ファイルの構築 \r\n","# #   学習済みモデルを作成するときに用いられた辞書ファイルがあるため必ずしも実行しなくてよい\r\n","# # build_dict()\r\n","\r\n","# # 言語モデルオブジェクトの生成\r\n","# ln=Language()\r\n","\r\n","# # 学習実行 + テストデータ評価, モデルファイル保存\r\n","# history = ln.train(\"./data_for_predict/trained_model\")"],"outputs":[],"metadata":{}}]}